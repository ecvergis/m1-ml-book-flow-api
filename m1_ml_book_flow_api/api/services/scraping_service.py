"""
M√≥dulo de servi√ßo para web scraping de livros.

Este m√≥dulo cont√©m fun√ß√µes para realizar web scraping do site books.toscrape.com,
extraindo informa√ß√µes sobre livros como t√≠tulo, pre√ßo, categoria, autor, rating, etc.
"""
import requests
from bs4 import BeautifulSoup
from typing import List, Dict
from urllib.parse import urljoin, urlparse
import re

# URLs base do site de scraping
BASE_URL = "https://books.toscrape.com"
CATALOGUE_URL = f"{BASE_URL}/catalogue"

def get_total_pages() -> int:
    """
    Obt√©m o n√∫mero total de p√°ginas dispon√≠veis para scraping.
    
    Returns:
        int: N√∫mero total de p√°ginas encontradas. Retorna 0 se n√£o conseguir determinar.
    """
    try:
        response = requests.get(f"{BASE_URL}/index.html", timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        pagination = soup.find('ul', class_='pager')
        if pagination:
            page_numbers = pagination.find_all('li', class_='current')
            if page_numbers:
                total_text = page_numbers[0].text.strip()
                import re
                match = re.search(r'of (\d+)', total_text)
                if match:
                    return int(match.group(1))
    except:
        pass
    return 0

def scrape_page(page: int, total_pages: int = 0) -> List[Dict]:
    """
    Processa uma √∫nica p√°gina e extrai dados dos livros.
    
    Args:
        page (int): N√∫mero da p√°gina a ser processada (1 para primeira p√°gina)
        total_pages (int, optional): Total de p√°ginas estimadas para exibi√ß√£o no log
        
    Returns:
        List[Dict]: Lista de dicion√°rios contendo dados dos livros extra√≠dos.
                   Retorna lista vazia se a p√°gina n√£o existir ou n√£o contiver livros.
                   
    Cada dicion√°rio cont√©m:
        - title: T√≠tulo do livro
        - author: Nome do autor
        - year: Ano de publica√ß√£o
        - category: Categoria do livro
        - price: Pre√ßo em float
        - rating: Avalia√ß√£o em estrelas (float de 1.0 a 5.0)
        - available: Se est√° dispon√≠vel (boolean)
        - image: URL da imagem da capa
    """
    import logging
    logger = logging.getLogger(__name__)
    
    if page == 1:
        url = f"{BASE_URL}/index.html"
    else:
        url = f"{CATALOGUE_URL}/page-{page}.html"
    
    try:
        print(f"\nüìÑ Processando p√°gina {page}/{total_pages if total_pages > 0 else '?'}...")
        logger.info(f"Scraping page {page}/{total_pages if total_pages > 0 else '?'}...")
        
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        books = soup.find_all('article', class_='product_pod')
        
        if not books:
            print(f"‚ö†Ô∏è  Nenhum livro encontrado na p√°gina {page}")
            logger.info(f"No books found on page {page}")
            return []
        
        print(f"‚úÖ Encontrados {len(books)} livros na p√°gina {page}")
        logger.info(f"Found {len(books)} books on page {page}")
        
        books_data = []
        for i, book in enumerate(books, 1):
            book_data = extract_book_data(book, BASE_URL)
            if book_data:
                books_data.append(book_data)
            if i % 10 == 0:
                print(f"  ‚è≥ Processados {i}/{len(books)} livros da p√°gina")
                logger.info(f"Processed {i}/{len(books)} books from page {page}")
        
        print(f"‚úÖ P√°gina {page} processada! {len(books_data)} livros extra√≠dos")
        logger.info(f"Page {page} completed. Extracted {len(books_data)} books")
        
        return books_data
        
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro ao buscar p√°gina {page}: {e}")
        logger.error(f"Error fetching page {page}: {e}")
        return []

def has_next_page(page: int) -> bool:
    """
    Verifica se existe uma pr√≥xima p√°gina para processar.
    
    Args:
        page (int): N√∫mero da p√°gina atual
        
    Returns:
        bool: True se existe pr√≥xima p√°gina, False caso contr√°rio
    """
    try:
        if page == 1:
            url = f"{BASE_URL}/index.html"
        else:
            url = f"{CATALOGUE_URL}/page-{page}.html"
        
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        next_button = soup.find('li', class_='next')
        return next_button is not None
    except:
        return False

def extract_book_data(book_element, base_url: str) -> Dict:
    """
    Extrai dados de um √∫nico livro a partir do elemento HTML.
    
    Esta fun√ß√£o busca na p√°gina de listagem e na p√°gina de detalhes do livro
    para extrair todas as informa√ß√µes dispon√≠veis.
    
    Args:
        book_element: Elemento BeautifulSoup representando um artigo de livro
        base_url (str): URL base do site para constru√ß√£o de URLs absolutas
        
    Returns:
        Dict: Dicion√°rio com dados do livro ou None se n√£o conseguir extrair dados essenciais.
    """
    try:
        # Extrai t√≠tulo do livro
        title_elem = book_element.find('h3')
        title = title_elem.find('a')['title'] if title_elem and title_elem.find('a') else None
        
        # Extrai pre√ßo do livro
        price_elem = book_element.find('p', class_='price_color')
        price_str = price_elem.text.strip() if price_elem else "¬£0.00"
        price = parse_price(price_str)
        
        # Extrai avalia√ß√£o (rating em estrelas)
        rating_elem = book_element.find('p', class_='star-rating')
        rating = parse_rating(rating_elem) if rating_elem else None
        
        # Extrai disponibilidade do estoque
        availability_elem = book_element.find('p', class_='instock')
        available = True
        if availability_elem:
            availability_text = availability_elem.text.strip().lower()
            available = 'in stock' in availability_text
        
        # Extrai URL da imagem da capa
        image_elem = book_element.find('img')
        image_url = None
        if image_elem and image_elem.get('src'):
            image_relative = image_elem['src']
            image_url = urljoin(base_url, image_relative)
        
        # Para obter categoria, autor e outros detalhes, precisa visitar a p√°gina de detalhes do livro
        link_elem = book_element.find('h3')
        book_detail_url = None
        if link_elem and link_elem.find('a'):
            book_detail_relative = link_elem.find('a')['href']
            # O href pode ser relativo ao cat√°logo ou absoluto
            if book_detail_relative.startswith('..'):
                book_detail_url = urljoin(BASE_URL + '/', book_detail_relative.replace('../', ''))
            elif book_detail_relative.startswith('catalogue/'):
                book_detail_url = urljoin(BASE_URL + '/', book_detail_relative)
            else:
                book_detail_url = urljoin(CATALOGUE_URL + '/', book_detail_relative)
        
        category = None
        author = None
        year = None
        
        # Busca a p√°gina de detalhes do livro para obter categoria, autor e ano
        if book_detail_url:
            try:
                detail_response = requests.get(book_detail_url, timeout=5)
                detail_response.raise_for_status()
                detail_soup = BeautifulSoup(detail_response.content, 'html.parser')
                
                # Extrai categoria do breadcrumb (navega√ß√£o)
                breadcrumb = detail_soup.find('ul', class_='breadcrumb')
                if breadcrumb:
                    category_links = breadcrumb.find_all('a')
                    # Se h√° 3 ou mais links, pega o terceiro (categoria espec√≠fica como Romance)
                    # Se h√° apenas 2 links, pega o segundo (Books)
                    if len(category_links) >= 3:
                        category = category_links[2].text.strip()
                    elif len(category_links) >= 2:
                        category = category_links[1].text.strip()
                
                # Extrai autor da tabela de informa√ß√µes do produto
                product_info = detail_soup.find('article', class_='product_page')
                if product_info:
                    table = product_info.find('table', class_='table')
                    if table:
                        rows = table.find_all('tr')
                        for row in rows:
                            th = row.find('th')
                            td = row.find('td')
                            if th and td:
                                if 'author' in th.text.lower():
                                    author = td.text.strip()
                                elif 'number of pages' in th.text.lower():
                                    # Tentativa de extrair ano da descri√ß√£o ou outros campos
                                    pass
                
            except Exception as e:
                print(f"Erro ao buscar detalhes do livro em {book_detail_url}: {e}")
        
        # Valida√ß√£o: t√≠tulo e pre√ßo s√£o obrigat√≥rios
        if not title or price is None:
            return None
        
        return {
            'title': title,
            'author': author,
            'year': year,
            'category': category,
            'price': price,
            'rating': rating,
            'available': available,
            'image': image_url
        }
        
    except Exception as e:
        print(f"Erro ao extrair dados do livro: {e}")
        return None

def parse_price(price_str: str) -> float:
    """
    Converte string de pre√ßo para float.
    
    Remove s√≠mbolos de moeda e extrai apenas o valor num√©rico.
    Exemplo: '¬£51.77' -> 51.77
    
    Args:
        price_str (str): String com o pre√ßo (ex: '¬£51.77')
        
    Returns:
        float: Pre√ßo como n√∫mero float. Retorna 0.0 em caso de erro.
    """
    try:
        # Remove s√≠mbolos de moeda e extrai apenas o n√∫mero
        price_clean = re.sub(r'[^\d.]', '', price_str)
        return float(price_clean)
    except:
        return 0.0

def parse_rating(rating_elem) -> float:
    """
    Converte elemento de avalia√ß√£o em estrelas para n√∫mero float.
    
    Converte classes CSS como 'Three' para o valor num√©rico correspondente.
    Exemplo: 'Three' -> 3.0
    
    Args:
        rating_elem: Elemento BeautifulSoup com classes de avalia√ß√£o
        
    Returns:
        float: Avalia√ß√£o de 1.0 a 5.0 ou None se n√£o conseguir determinar
    """
    rating_map = {
        'One': 1.0,
        'Two': 2.0,
        'Three': 3.0,
        'Four': 4.0,
        'Five': 5.0
    }
    
    rating_classes = rating_elem.get('class', [])
    for cls in rating_classes:
        if cls in rating_map:
            return rating_map[cls]
    
    return None

